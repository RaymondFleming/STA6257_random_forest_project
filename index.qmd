---
title: "Random Forest Algorithm"
author: "Kristen Monaco, Praya Cheekapara, Raymond Fleming, Teng Ma"
date: '`r Sys.Date()`'
format:
  html:
    code-fold: true
course: STA 6257 - Advanced Statistical Modeling
bibliography: references.bib # file contains bibtex for references
#always_allow_html: true # this allows to get PDF with HTML features
self-contained: true
execute: 
  warning: false
  message: false
editor: 
  markdown: 
    wrap: 72
---

# Introduction
Random Forest is an ensemble learning method that combines multiple decision trees to make predictions.
The main idea behind Random Forest is to introduce randomness in the training process by creating a diverse set of decision trees. Each tree in the Random Forest is trained on a random subset of the data and a random subset of features. By combining the predictions of multiple trees, Random Forest reduces the risk of overfitting and improves the overall accuracy and robustness of the model. The randomness in the training process also helps in handling high-dimensional data, handling missing values, and providing estimates of variable importance. Random Forest has gained popularity due to its ability to handle complex datasets and produce reliable predictions in various domains.

# Literature Review
The thesis [@arXiv:1407.7502] dives deep into a detailed analysis of random forests, which is an important machine learning algorithm. It aims to understand how they learn, how they work internally, and how easy they are to interpret. In the first part, it explores the creation of decision trees and their assembly into random forests, including their design and purpose. It also presents a study on the computational efficiency and scalability of random forests, with specific implementation insights from Scikit-Learn. The second part focuses on understanding how interpretable random forests are by examining the Mean Decrease Impurity measure - a key method for determining variable importance, especially in the context of multiway totally randomized trees under extensive or asymptotic conditions.

[@arXiv:2304.02490]

[@arXiv:2401.12667]

[@arXiv:2401.10959]



# Methods

# Analysis and Results

# Data clean

## Packages
The R packages utilized for running the statistical modeling for the Random Forest algorithm were **readr**, **plyr**, **ipred**, **caret**, **caTools**, **randomForest**, **ROSE**. 
- readr: Part of the tidyverse, readr is designed for reading rectangular data, particularly CSVs (comma-separated values) and other delimited types of text files. It's known for its speed and for providing more informative error messages compared to base R functions like read.csv. It also converts data into tibbles, which are a modern take on data frames.

- plyr: This package is used for splitting, applying, and combining data. plyr is known for its capability to handle different data types (arrays, lists, data frames, etc.) and apply functions to each element of the split data, then combine the results. Note that plyr is largely superseded by dplyr (also part of tidyverse), which is more efficient especially for large datasets.

- ipred: Standing for "Improved Predictors", ipred provides functions for predictive modeling. It includes methods for bagging (Bootstrap Aggregating), which helps improve the stability and accuracy of machine learning algorithms, particularly for decision trees.

- caret: The caret package (short for Classification And REgression Training) is a comprehensive framework for building machine learning models in R. It simplifies the process of model training, tuning, and predicting by providing a unified interface for various machine learning algorithms.

- caTools: This package contains several tools for handling data, including functions for reading/writing Binary Large Objects (BLOBs), moving window statistics, and splitting data into training/testing sets. It's often used for its simple and effective method for creating reproducible train/test splits.

- randomForest: As the name suggests, this package is used for implementing the Random Forest algorithm for classification and regression tasks. Random Forest is an ensemble learning method that operates by constructing a multitude of decision trees and outputting the mode of the classes (classification) or mean prediction (regression) of the individual trees.

- ROSE: Standing for Random OverSampling Examples, the ROSE package is used to deal with imbalanced datasets in binary classification problems. It generates synthetic samples in a two-class problem to balance the class distribution, using smoothed bootstrapping. This helps improve the performance of classification models on imbalanced datasets.

### Import packages
```{r}
library(readr)
library(plyr)
library(ipred)
library(caret)
library(caTools)
library(randomForest)
library(ROSE)
```
## Data and Visualization

A study was conducted to determine how...

```{r, warning=FALSE, echo=T, message=FALSE}

```

```{r, warning=FALSE, echo=TRUE}

```

## Statistical Modeling

```{r}

```

## Conclusion

## References
