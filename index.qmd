---
title: "Random Forest Algorithm"
author: "Kristen Monaco, Praya Cheekapara, Raymond Fleming, Teng Ma"
date: '`r Sys.Date()`'
format:
  html:
    code-fold: true
course: STA 6257 - Advanced Statistical Modeling
bibliography: references.bib # file contains bibtex for references
#always_allow_html: true # this allows to get PDF with HTML features
self-contained: true
execute: 
  warning: false
  message: false
editor: 
  markdown: 
    wrap: 72
---

# Introduction
Random Forest is a common ensemble method used to make predictions from a variety of dataset types. This is done by combining a number of diverse, but simpler decision tree models into an ensemble model which can be more accurate, but also more versatile.   In creating a normal random forest, each of the decision trees is created with a randomized subset of data, and a random subset of features, with the output of each tree combined into the output of the random forest. The benefits to building the model this way include the ability to handle missing data natively, dimensionality reduction or reducing the need to perform dimensionality reduction prior to modeling, and the structure of the forest can be used to estimate variable importance, which can then be utilized in training other models or feature engineering.  Given these benefits, and the ability to use complex datasets natively, Random Forests have gained popularity over time and are often used.

## Random Forest involves the following basic concepts:
1. **Bootstrap Sampling (Bagging):**
Perform random sampling with replacement on the original dataset to form a new dataset for training a decision tree. In each round of bootstrap sampling, about 36.8% of the samples will be missed, not appearing in the new dataset, and these data are referred to as out-of-bag (OOB) data.

2. **Random Feature Selection:**
At each node of the decision tree training, randomly select a subset of features, and then use information gain (or other criteria) to choose the best split. 
Repeat the above steps, generating multiple decision trees, to form a "forest".

3. **Prediction:**
When predicting new data samples, each tree produces its own prediction result. Random Forest synthesizes these results and uses majority voting to determine the final prediction outcome.

4. **Ensemble Learning via Hard Voting Classifier:**
By voting, the results of five models are integrated to get the combined outcome. The result selects the category that appears most frequently as the final prediction result. This is a strategy of ensemble learning, known as the Hard Voting Classifier.
For a sample point x, five models make predictions respectively:

> y1 = Model1(x)
> 
> y2 = Model2(x)
> 
> y3 = Model3(x)
> 
> y4 = Model4(x)
> 
> y5 = Model5(x)

Put these five prediction results into a set `Y = {y1, y2, y3, y4, y5}`, and the final prediction result y_final is the element that appears most frequently in the set Y, mathematically represented as:

> y_final = mode(Y)

# Literature Review
The thesis [@arXiv:1407.7502] dives deep into a detailed analysis of random forests, which is an important machine learning algorithm. It aims to understand how they learn, how they work internally, and how easy they are to interpret. In the first part, it explores the creation of decision trees and their assembly into random forests, including their design and purpose. It also presents a study on the computational efficiency and scalability of random forests, with specific implementation insights from Scikit-Learn. The second part focuses on understanding how interpretable random forests are by examining the Mean Decrease Impurity measure - a key method for determining variable importance, especially in the context of multiway totally randomized trees under extensive or asymptotic conditions.

Random Forest is a widely used machine learning method for analyzing high-dimensional data, loved for its flexibility and ability to identify important features. However, it often overlooks the intricate connections among features and how they collectively influence outcomes. The thesis [@arXiv:2304.02490] introduces two innovative approaches that tackle this issue: Mutual Forest Impact (MFI) and Mutual Impurity Reduction (MIR). MFI assesses the combined effect of features on outcomes, providing a more detailed understanding than correlation analysis. MIR takes this even further by integrating the relationship parameter with individual feature importance, incorporating testing procedures for selecting features based on statistical significance. Evaluations on simulated datasets and comparisons with existing feature selection methods demonstrate the potential of MFI and MIR in uncovering complex relationships between features and outcomes without any biases towards favoring features with many splits or high minor allele frequencies.

The paper [@arXiv:2401.12667] introduces a new method for selecting features called ROBUST Weighted Score for Unbalanced data (ROWSU). It's specifically designed to deal with the problem of class imbalance in high-dimensional gene expression data when doing binary classification tasks. To tackle the challenge of imbalanced classes, ROWSU first balances out the dataset by creating synthetic data points for the minority class. Then it uses a greedy search to find the smallest set of genes that are important, and it introduces a unique weighted robust score that calculates how useful each gene is using support vector weights. This whole process results in a final set of genes that combines both high-scoring genes and those found through greedy search, making sure we select genes that can really tell our classes apart even if they're imbalanced. We tested ROWSU on six different gene expression datasets and compared its performance to other state-of-the-art feature selection techniques using accuracy and sensitivity metrics. We also visualized our results with boxplots and stability plots. Our findings show that ROWSU performs better than other methods at improving classifier effectiveness, as shown by its results with k nearest neighbors (kNN) and random forest (RF) classifiers.

The main focus of this article [@arXiv:2401.10959] is to make sure that the electrical grid works properly by making energy providers follow the rules and specifications set by Transmission System Operators (TSOs). Since there are many different energy sources connected through power electronic inverters, it's crucial for them to choose between Grid Forming (GFM) and Grid Following (GFL) operating modes in order to maintain grid stability. This means that energy suppliers need to comply with these requirements. In this study, we compare various machine learning algorithms to classify converter control modes (GFL or GFM) using frequency-domain admittance from external measurements. While most algorithms can accurately identify known control structures, they struggle when faced with new modifications. However, the random forest algorithm stands out as it consistently performs well across different control configurations.

Biau [@biau2012analysis] delves into the statistical components and mathematical support of the random forest model. Several theorems are outlined in which to display the consistency of the model. Then, proof is provided for those outcomes, with worked out equations that adjust for inequalities and support the propositions. He touches in the importance of variable selection and the increasing adaptability found within the random forest algorithm.

Biau and Scornet [@biau2016random] provide an overview into the methodology, practice, and recent developments of the random forest algorithm. Basic principles are discussed, and mathematical support for the algorithm is provided. They delve into the importance of variable selection and tree parameters when performing predictive analysis.

Liaw and Wiener [@liaw2002classification] discuss the introduction of the random forest algorithm by L. Breiman, in addition to offering examples of its application in the R interface. They discuss how the algorithm draws bootstrap samples and estimates rates of error. It is noted that the production of multiple trees is crucial in obtaining variable importance and measures of proximity.

Lingjun et al. [@lingjun2019random] discuss uses and advantages of tree-based machine learning algorithms. They highlight the key benefits of these models over classical regression analyses, and they expound upon the predictive capabilities as a strategy for data-based decision making. A simulation experiment is outlined to allow for greater understanding of the process and its corresponding results.

Segal [@segal2004machine] offers a clear definition and practical application of random forest regression. Two different profiles are established using random forest methodology, and predictive errors are identified. Mathematical equations lend support to the regression analysis. Segal (2004) explains that one of guiding forces in random forest regression is to increase variance by decreasing correlation.

# Analysis and Results
## Packages
The R packages utilized for running the statistical modeling for the Random Forest algorithm were **readr**, **plyr**, **ipred**, **caret**, **caTools**, **randomForest**, **ROSE**. 
- readr: Part of the tidyverse, readr is designed for reading rectangular data, particularly CSVs (comma-separated values) and other delimited types of text files. It's known for its speed and for providing more informative error messages compared to base R functions like read.csv. It also converts data into tibbles, which are a modern take on data frames.

- plyr: This package is used for splitting, applying, and combining data. plyr is known for its capability to handle different data types (arrays, lists, data frames, etc.) and apply functions to each element of the split data, then combine the results. Note that plyr is largely superseded by dplyr (also part of tidyverse), which is more efficient especially for large datasets.

- ipred: Standing for "Improved Predictors", ipred provides functions for predictive modeling. It includes methods for bagging (Bootstrap Aggregating), which helps improve the stability and accuracy of machine learning algorithms, particularly for decision trees.

- caret: The caret package (short for Classification And REgression Training) is a comprehensive framework for building machine learning models in R. It simplifies the process of model training, tuning, and predicting by providing a unified interface for various machine learning algorithms.

- caTools: This package contains several tools for handling data, including functions for reading/writing Binary Large Objects (BLOBs), moving window statistics, and splitting data into training/testing sets. It's often used for its simple and effective method for creating reproducible train/test splits.

- randomForest: As the name suggests, this package is used for implementing the Random Forest algorithm for classification and regression tasks. Random Forest is an ensemble learning method that operates by constructing a multitude of decision trees and outputting the mode of the classes (classification) or mean prediction (regression) of the individual trees.

- ROSE: Standing for Random OverSampling Examples, the ROSE package is used to deal with imbalanced datasets in binary classification problems. It generates synthetic samples in a two-class problem to balance the class distribution, using smoothed bootstrapping. This helps improve the performance of classification models on imbalanced datasets.

### Import packages
```{r}
library(readr)
library(plyr)
library(ipred)
library(caret)
library(caTools)
library(randomForest)
library(ROSE)
library(ggplot2)
library(knitr)
```
## Data and Visualization

The preview of the dataset

```{r, warning=FALSE, echo=T, message=FALSE}

data <- read_csv("All_threat_data.csv")

ggplot(data, aes(x = factor(Status), fill = factor(Status))) + 
  geom_bar(show.legend = FALSE) +
  scale_fill_brewer(palette = "Paired") +
  labs(title = "Barplot of Status", 
       x = "Status", 
       y = "Frequency") +
  theme_minimal() +
  theme(text = element_text(size = 12),
        plot.title = element_text(hjust = 0.5),
        axis.title = element_text(size = 14, face = "bold"),
        axis.text.x = element_text(angle = 45, hjust = 1),
        panel.grid.major = element_line(color = "grey80"),
        panel.grid.minor = element_blank())
```

## Data Preparation
Encode the 15 columns, with 'group' categorized into 1, 2, and 3. 'Yes' should be encoded as 1, 'No' as 0, and other categories should be numbered starting from 1. In the first column, replace the "-" with a ".".

### Original Dataset Preview

```{r}
original_data <- read_csv("All_threat_data.csv")
kable(head(original_data, 10))
```
### Encoding Dataset Prview

```{r, warning=FALSE, echo=TRUE}
encode_data <- read_csv("process.csv")
kable(head(encode_data, 10))
```
Split the dataset into a training set and a test set with a ratio of 7:3. Perform class imbalance handling on the training set using the ROSE library, aiming for an equal data quantity among classes, represented as 1:1:1.

Given a minority class sample point x, we find its k nearest neighbors. Then, we randomly select one of these neighbors, denoted as z, and construct a new data point y that lies on the line segment between x and z:

> y = x + λ * (z - x)

Here, λ is a random number between 0 and 1.

## Methods
The data needs to be normalized using 5 different methods (Min-Max Normalization, Z-Score Normalization, Max Absolute Scaling, L1 Norm Normalization, L2 Norm Normalization) due to significant differences in feature values initially.

> X_new = (X_old-min(X_old)) / (max(X_old)-min(X_old))
>
> X_new = (X_old - mean(X_old)) / sd(X_old)
>
> X_new = X_old / max(abs(X_old))
>
> X_new = X_old / sum(abs(X_old))
>
> X_new = X_old / sqrt(sum(X_old^2))

## Evaluation
Evaluate the test set and calculate four metrics (accuracy, recall, precision, F1).

- Accuracy = sum(actual labels == predicted labels) / total number of labels
- Recall = True Positives / (True Positives + False Negatives)
- Precision = True Positives / (True Positives + False Positives)
- F1 = 2 * (Precision * Recall) / (Precision + Recall)

## Final outcome:
The idea is to combine five models using a voting method and pick the category with the most votes as our final prediction. It's like a team effort in machine learning, known as the Hard Voting Classifier.
For a sample point x, five models make predictions respectively:

> y1 = Model1(x)
> 
> y2 = Model2(x)
> 
> y3 = Model3(x)
> 
> y4 = Model4(x)
> 
> y5 = Model5(x)

Place these five prediction results into a set `Y = {y1, y2, y3, y4, y5}`. The final prediction result `y_final` is the element that appears most frequently in the set `Y`, mathematically represented as `y_final = mode(Y)`.

## Statistical Modeling
### Data Processing
1. Process the data by setting the first 14 columns as [features] and the last column as the [label]
2. Split the dataset into training and testing sets
3. Combine the training datasets
4. Print the initial number of each category

```{r}
data <- read.csv("process.csv")

features <- data[, 1:14]
label <- data[, 15]

set.seed(42)

split <- sample.split(label, SplitRatio = 0.7)
features_train = features[split,]
features_test = features[!split,]
label_train = label[split]
label_test = label[!split]

data_train <- features_train
data_train$label <- label_train
class_counts <- table(data_train$label)

class_counts <- table(data_train$label)
print(paste("( Before )Data Category Counts: ", class_counts))
```
#### Handle class imbalance
1. Process classes A and B
2. Process classes A and C
3. Retain records in data_train_AB_resampled where the label is '2'
4. Retain records in data_train_AC_resampled where the label is '3'
5. Retain records in both data_train_AB_resampled and data_train_AC_resampled where the label is '1'
6. combine
7. Print the number of each category after class imbalance handling

```{r}
data_train_AB <- data_train
data_train_AB <- data_train_AB[data_train_AB$label != '3',]
data_train_AB_resampled <- ovun.sample(label ~ ., data = data_train_AB, method = "over", N = 980, seed = 1)$data

data_train_AC <- data_train
data_train_AC <- data_train_AC[data_train_AC$label != '2',]
data_train_AC_resampled <- ovun.sample(label ~ ., data = data_train_AC, method = "over", N = 980, seed = 1)$data

data_train_AB_2 <- data_train_AB_resampled[data_train_AB_resampled$label == '2',]
data_train_AC_3 <- data_train_AC_resampled[data_train_AC_resampled$label == '3',]

data_train_1 <- data_train_AB_resampled[data_train_AB_resampled$label == '1',]
data_train_combined <- rbind(data_train_1, data_train_AB_2, data_train_AC_3)

cat("( After )Data Category Counts:\n")
print(table(data_train_combined$label))
```
#### Normalization
1. Divide the features and label, and apply different normalization to the training and testing sets
2. Apply Min-Max normalization to features_train and features_test 
3. Apply Z-Score normalization to each column of features_train
4. features_test <- as.data.frame(mapply(function(x, y) {(x - mean(y))/sd(y)}, features_test, features_train, SIMPLIFY = FALSE))
5. Apply Max Absolute Value normalization to the training set
6. Apply L1 norm normalization to the training set
7. Apply L2 norm normalization to the training set

```{r}
features_train <- data_train_combined[, 1:14]
label <- data_train_combined[, 15]
```

```{r}
features_train <- as.data.frame(lapply(features_train, function(x) {(x-min(x))/(max(x)-min(x))}))
features_test <- as.data.frame(lapply(features_test, function(x) {(x-min(x))/(max(x)-min(x))}))

features_train_1 <- as.data.frame(lapply(features_train, function(x) {(x-min(x))/(max(x)-min(x))}))
features_test_1 <- as.data.frame(lapply(features_test, function(x) {(x-min(x))/(max(x)-min(x))}))

features_train_2 <- as.data.frame(lapply(features_train, function(x) {(x - mean(x))/sd(x)}))
features_test_2 <- as.data.frame(lapply(features_test, function(x) {(x - mean(x))/sd(x)}))

features_train_3 <- as.data.frame(lapply(features_train, function(x) {x / max(abs(x))}))
features_test_3 <- as.data.frame(lapply(features_test, function(x) {x / max(abs(x))}))

features_train_4 <- as.data.frame(lapply(features_train, function(x) {x / sum(abs(x))}))
features_test_4 <- as.data.frame(lapply(features_test, function(x) {x / sum(abs(x))}))

features_train_5 <- as.data.frame(lapply(features_train, function(x) {x / sqrt(sum(x^2))}))
features_test_5 <- as.data.frame(lapply(features_test, function(x) {x / sqrt(sum(x^2))}))
```
#### Train dataset
1. Training dataset after Min-Max normalization
2. Training dataset after Min-Max normalization
3. Training dataset after Z-Score normalization
4. Training dataset after Max Absolute Value normalization
5. Training dataset after L1 norm normalization
6. Training dataset after L2 norm normalization

```{r}
data_train <- features_train
data_train$label <- label
class_counts <- table(data_train$label)

data_train_1 <- features_train_1
data_train_1$label <- label
class_counts_1 <- table(data_train_1$label)

data_train_2 <- features_train_2
data_train_2$label <- label
class_counts_2 <- table(data_train_2$label)

data_train_3 <- features_train_3
data_train_3$label <- label
class_counts_3 <- table(data_train_3$label)

data_train_4 <- features_train_4
data_train_4$label <- label
class_counts_4 <- table(data_train_4$label)

data_train_5 <- features_train_5
data_train_5$label <- label
class_counts_5 <- table(data_train_5$label)
```
#### Model 1
1. Calculate and print the accuracy of the test set
2. Convert to a categorical variable
3. Obtain the confusion matrix
4. Calculate the average recall rate (Sensitivity)
5. Calculate the average precision rate (Positive Predictive Value)
6. print results

```{r}
model_1 <- randomForest(x = data_train_1[-ncol(data_train_combined)], y = as.factor(data_train_1$label), ntree = 2)
variable_importance_1 = importance(model_1)
pred_comb_1 <- predict(model_1, features_test_1)

accuracy_1 <- sum(label_test == pred_comb_1) / length(label_test)
print(paste('Accuracy of Min-Max Normalization for model1:', accuracy_1))

label_test_factor <- as.factor(label_test)
pred_comb_1_factor <- as.factor(pred_comb_1)

cm <- confusionMatrix(pred_comb_1_factor, label_test_factor)

recall <- mean(c(cm$byClass["Class: 1", "Sensitivity"], cm$byClass["Class: 2", "Sensitivity"], cm$byClass["Class: 3", "Sensitivity"]))

precision <- mean(c(cm$byClass["Class: 1", "Pos Pred Value"], cm$byClass["Class: 2", "Pos Pred Value"], cm$byClass["Class: 3", "Pos Pred Value"]))
F1 = 2 * recall * precision / ( recall + precision )

print(paste('Recall of model1 :', recall))
print(paste('Precision of model1 :', precision))
print(paste('F1 of model1 :', F1))
```

#### Model 2
1. Calculate and print the accuracy of the test set
2. Convert to a factor
3. Obtain the confusion matrix
4. Calculate the average recall rate (Sensitivity)
5. Calculate the average precision rate (Positive Predictive Value)
6. print results

```{r}
model_2 <- randomForest(x = data_train_2[-ncol(data_train_combined)], y = as.factor(data_train_2$label), ntree = 2) # nolint
variable_importance_2 = importance(model_2) # nolint
pred_comb_2 <- predict(model_2, features_test_2)

accuracy_2 <- sum(label_test == pred_comb_2) / length(label_test)
print(paste('Z-Score Normalization Accuracy for model2:', accuracy_2)) # nolint

label_test_factor <- as.factor(label_test)
pred_comb_1_factor <- as.factor(pred_comb_2)

cm <- confusionMatrix(pred_comb_1_factor, label_test_factor)

recall <- mean(c(cm$byClass["Class: 1", "Sensitivity"],  # nolint
                              cm$byClass["Class: 2", "Sensitivity"],  # nolint
                              cm$byClass["Class: 3", "Sensitivity"]))

precision <- mean(c(cm$byClass["Class: 1", "Pos Pred Value"], # nolint 
                            cm$byClass["Class: 2", "Pos Pred Value"], # nolint 
                            cm$byClass["Class: 3", "Pos Pred Value"]))
F1 = 2 * recall * precision / ( recall + precision ) # nolint

print(paste('Recall of model2 :', recall))  # nolint
print(paste('Precision of model2 :', precision))  # nolint
print(paste('F1 of model2 :', F1))  # nolint
```
#### Model 3
1. Calculate and print the accuracy of the test set
2. Convert to a factor
3. Obtain the confusion matrix
4. Calculate the average recall rate (Sensitivity)
5. Calculate the average precision rate (Positive Predictive Value)
6. print results

```{r}
model_3 <- randomForest(x = data_train_3[-ncol(data_train_combined)], y = as.factor(data_train_3$label), ntree = 2)
variable_importance_3 = importance(model_3)
pred_comb_3 <- predict(model_3, features_test_3)

accuracy_3 <- sum(label_test == pred_comb_3) / length(label_test)
print(paste('Accuracy of Max absolute value normalization for model3:', accuracy_3))

label_test_factor <- as.factor(label_test)
pred_comb_1_factor <- as.factor(pred_comb_3)

cm <- confusionMatrix(pred_comb_1_factor, label_test_factor)

recall <- mean(c(cm$byClass["Class: 1", "Sensitivity"],
                              cm$byClass["Class: 2", "Sensitivity"],
                              cm$byClass["Class: 3", "Sensitivity"]))

precision <- mean(c(cm$byClass["Class: 1", "Pos Pred Value"],
                            cm$byClass["Class: 2", "Pos Pred Value"],
                            cm$byClass["Class: 3", "Pos Pred Value"]))
F1 = 2 * recall * precision / ( recall + precision )

print(paste('Recall of model3 :', recall))
print(paste('Precision of model3 :', precision))
print(paste('F1 of mode3 :', F1))
```

#### Model 4
1. Calculate and print the accuracy of the test set
2. Convert to a factor
3. Obtain the confusion matrix
4. Calculate the average recall rate (Sensitivity)
5. Calculate the average precision rate (Positive Predictive Value)
6. print results

```{r}
model_4 <- randomForest(x = data_train_4[-ncol(data_train_combined)], y = as.factor(data_train_4$label), ntree = 2)
variable_importance_4 = importance(model_4)
pred_comb_4 <- predict(model_4, features_test_4)

accuracy_4 <- sum(label_test == pred_comb_4) / length(label_test)
print(paste('Accuracy of L1 norm normalization for model4:', accuracy_4))

label_test_factor <- as.factor(label_test)
pred_comb_1_factor <- as.factor(pred_comb_4)

cm <- confusionMatrix(pred_comb_1_factor, label_test_factor)

recall <- mean(c(cm$byClass["Class: 1", "Sensitivity"],
                              cm$byClass["Class: 2", "Sensitivity"],
                              cm$byClass["Class: 3", "Sensitivity"]))

precision <- mean(c(cm$byClass["Class: 1", "Pos Pred Value"],
                            cm$byClass["Class: 2", "Pos Pred Value"],
                            cm$byClass["Class: 3", "Pos Pred Value"]))
F1 = 2 * recall * precision / ( recall + precision )

print(paste('Recall of model4 :', recall))
print(paste('Precision of model4 :', precision))
print(paste('F1 of model4 :', F1))
```
##### Model 5
1. Calculate and print the accuracy of the test set
2. Convert to a factor
3. Obtain the confusion matrix
4. Calculate the average recall rate (Sensitivity)
5. Calculate the average precision rate (Positive Predictive Value)
6. print results

```{r}
model_5 <- randomForest(x = data_train_5[-ncol(data_train_combined)], y = as.factor(data_train_5$label), ntree = 2)
variable_importance_5 = importance(model_5)
pred_comb_5 <- predict(model_5, features_test_5)

accuracy_5 <- sum(label_test == pred_comb_5) / length(label_test)
print(paste('Accuracy of L2 norm normalization for model5:', accuracy_5))

label_test_factor <- as.factor(label_test)
pred_comb_1_factor <- as.factor(pred_comb_5)

cm <- confusionMatrix(pred_comb_1_factor, label_test_factor)

recall <- mean(c(cm$byClass["Class: 1", "Sensitivity"],
                              cm$byClass["Class: 2", "Sensitivity"],
                              cm$byClass["Class: 3", "Sensitivity"]))

precision <- mean(c(cm$byClass["Class: 1", "Pos Pred Value"],
                            cm$byClass["Class: 2", "Pos Pred Value"],
                            cm$byClass["Class: 3", "Pos Pred Value"]))
F1 = 2 * recall * precision / ( recall + precision )

print(paste('Recall of model5 :', recall))
print(paste('Precision of model5 :', precision))
print(paste('F1 of model5 :', F1))
```
#### Prediction
1. Obtain the number of predicted results
2. Initialize an empty vector to store the final prediction results
3. Iterate over each test sample
4. Get the prediction results of the five models for the i-th sample 
5. Select the most frequently predicted class as the final prediction result for the i-th sample
6. Now final_pred contains the prediction results after voting
7. Calculate and print the accuracy
8. Convert to a factor type
9. Obtain the confusion matrix
10. Calculate the recall (Sensitivity) for each category
11. Calculate the precision for each category

```{r}

n <- length(pred_comb_1)

final_pred <- rep(NA, n)

for(i in 1:n) {
   preds <- c(pred_comb_1[i], pred_comb_2[i], pred_comb_3[i], pred_comb_4[i], pred_comb_5[i])

   final_pred[i] <- as.numeric(names(which.max(table(preds))))
}

importances_list <- list(variable_importance_1, variable_importance_2, variable_importance_3, variable_importance_4, variable_importance_5)
average_importance <- Reduce("+", importances_list) / length(importances_list)
print(average_importance)

accuracy <- sum(label_test == final_pred) / length(label_test)
print(paste('Accuracy of Voting method:', accuracy))

final_pred_factor <- as.factor(final_pred)
label_test_factor <- as.factor(label_test)

cm_vote <- confusionMatrix(final_pred_factor, label_test_factor)

sensitivity_class1 <- cm_vote$byClass["Class: 1", "Sensitivity"]
sensitivity_class2 <- cm_vote$byClass["Class: 2", "Sensitivity"]
sensitivity_class3 <- cm_vote$byClass["Class: 3", "Sensitivity"]
recall = (sensitivity_class1 + sensitivity_class2 + sensitivity_class3) / 3
print(paste('Recall :', recall))

precision_class1 <- cm_vote$byClass["Class: 1", "Pos Pred Value"]
precision_class2 <- cm_vote$byClass["Class: 2", "Pos Pred Value"]
precision_class3 <- cm_vote$byClass["Class: 3", "Pos Pred Value"]
precision = (precision_class1 + precision_class2 + precision_class3) / 3
print(paste('Precision :', precision))

F1 = 2 * recall * precision / ( recall + precision )
print(paste('F1 :', F1))
```
## Conclusion
# TODO
# TODO: Add references of Hard Voting Classifier
## References