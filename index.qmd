---
title: "Random Forest Algorithm"
author: "Kristen Monaco, Praya Cheekapara, Raymond Fleming, Teng Ma"
date: '`r Sys.Date()`'
format:
  html:
    code-fold: true
course: STA 6257 - Advanced Statistical Modeling
bibliography: references.bib # file contains bibtex for references
#always_allow_html: true # this allows to get PDF with HTML features
self-contained: true
execute: 
  warning: false
  message: false
editor: 
  markdown: 
    wrap: 72
---

# Introduction
Random Forest is a common ensemble method used to make predictions from a variety of dataset types. This is done by combining a number of diverse, but simpler decision tree models into an ensemble model which can be more accurate, but also more versatile.   In creating a normal random forest, each of the decision trees is created with a randomized subset of data, and a random subset of features, with the output of each tree combined into the output of the random forest. The benefits to building the model this way include the ability to handle missing data natively, dimensionality reduction or reducing the need to perform dimensionality reduction prior to modeling, and the structure of the forest can be used to estimate variable importance, which can then be utilized in training other models or feature engineering.  Given these benefits, and the ability to use complex datasets natively, Random Forests have gained popularity over time and are often used.

## Random Forest involves the following basic concepts:
1. **Bootstrap Sampling (Bagging):**
Perform random sampling with replacement on the original dataset to form a new dataset for training a decision tree. In each round of bootstrap sampling, about 36.8% of the samples will be missed, not appearing in the new dataset, and these data are referred to as out-of-bag (OOB) data.

2. **Random Feature Selection:**
At each node of the decision tree training, randomly select a subset of features, and then use information gain (or other criteria) to choose the best split. 
Repeat the above steps, generating multiple decision trees, to form a "forest".

3. **Prediction:**
When predicting new data samples, each tree produces its own prediction result. Random Forest synthesizes these results and uses majority voting to determine the final prediction outcome.

4. **Ensemble Learning via Hard Voting Classifier:**
By voting, the results of five models are integrated to get the combined outcome. The result selects the category that appears most frequently as the final prediction result. This is a strategy of ensemble learning, known as the Hard Voting Classifier.
For a sample point x, five models make predictions respectively:

> y1 = Model1(x)
> 
> y2 = Model2(x)
> 
> y3 = Model3(x)
> 
> y4 = Model4(x)
> 
> y5 = Model5(x)

Put these five prediction results into a set `Y = {y1, y2, y3, y4, y5}`, and the final prediction result y_final is the element that appears most frequently in the set Y, mathematically represented as:

> y_final = mode(Y)

# Literature Review
The thesis [@arXiv:1407.7502] dives deep into a detailed analysis of random forests, which is an important machine learning algorithm. It aims to understand how they learn, how they work internally, and how easy they are to interpret. In the first part, it explores the creation of decision trees and their assembly into random forests, including their design and purpose. It also presents a study on the computational efficiency and scalability of random forests, with specific implementation insights from Scikit-Learn. The second part focuses on understanding how interpretable random forests are by examining the Mean Decrease Impurity measure - a key method for determining variable importance, especially in the context of multiway totally randomized trees under extensive or asymptotic conditions.

Random Forest is a widely used machine learning method for analyzing high-dimensional data, loved for its flexibility and ability to identify important features. However, it often overlooks the intricate connections among features and how they collectively influence outcomes. The thesis [@arXiv:2304.02490] introduces two innovative approaches that tackle this issue: Mutual Forest Impact (MFI) and Mutual Impurity Reduction (MIR). MFI assesses the combined effect of features on outcomes, providing a more detailed understanding than correlation analysis. MIR takes this even further by integrating the relationship parameter with individual feature importance, incorporating testing procedures for selecting features based on statistical significance. Evaluations on simulated datasets and comparisons with existing feature selection methods demonstrate the potential of MFI and MIR in uncovering complex relationships between features and outcomes without any biases towards favoring features with many splits or high minor allele frequencies.

The paper [@arXiv:2401.12667] introduces a new method for selecting features called ROBUST Weighted Score for Unbalanced data (ROWSU). It's specifically designed to deal with the problem of class imbalance in high-dimensional gene expression data when doing binary classification tasks. To tackle the challenge of imbalanced classes, ROWSU first balances out the dataset by creating synthetic data points for the minority class. Then it uses a greedy search to find the smallest set of genes that are important, and it introduces a unique weighted robust score that calculates how useful each gene is using support vector weights. This whole process results in a final set of genes that combines both high-scoring genes and those found through greedy search, making sure we select genes that can really tell our classes apart even if they're imbalanced. We tested ROWSU on six different gene expression datasets and compared its performance to other state-of-the-art feature selection techniques using accuracy and sensitivity metrics. We also visualized our results with boxplots and stability plots. Our findings show that ROWSU performs better than other methods at improving classifier effectiveness, as shown by its results with k nearest neighbors (kNN) and random forest (RF) classifiers.

The main focus of this article [@arXiv:2401.10959] is to make sure that the electrical grid works properly by making energy providers follow the rules and specifications set by Transmission System Operators (TSOs). Since there are many different energy sources connected through power electronic inverters, it's crucial for them to choose between Grid Forming (GFM) and Grid Following (GFL) operating modes in order to maintain grid stability. This means that energy suppliers need to comply with these requirements. In this study, we compare various machine learning algorithms to classify converter control modes (GFL or GFM) using frequency-domain admittance from external measurements. While most algorithms can accurately identify known control structures, they struggle when faced with new modifications. However, the random forest algorithm stands out as it consistently performs well across different control configurations.

Biau [@biau2012analysis] delves into the statistical components and mathematical support of the random forest model. Several theorems are outlined in which to display the consistency of the model. Then, proof is provided for those outcomes, with worked out equations that adjust for inequalities and support the propositions. He touches in the importance of variable selection and the increasing adaptability found within the random forest algorithm.

Biau and Scornet [@biau2016random] provide an overview into the methodology, practice, and recent developments of the random forest algorithm. Basic principles are discussed, and mathematical support for the algorithm is provided. They delve into the importance of variable selection and tree parameters when performing predictive analysis.

Liaw and Wiener [@liaw2002classification] discuss the introduction of the random forest algorithm by L. Breiman, in addition to offering examples of its application in the R interface. They discuss how the algorithm draws bootstrap samples and estimates rates of error. It is noted that the production of multiple trees is crucial in obtaining variable importance and measures of proximity.

Lingjun et al. [@lingjun2019random] discuss uses and advantages of tree-based machine learning algorithms. They highlight the key benefits of these models over classical regression analyses, and they expound upon the predictive capabilities as a strategy for data-based decision making. A simulation experiment is outlined to allow for greater understanding of the process and its corresponding results.

Segal [@segal2004machine] offers a clear definition and practical application of random forest regression. Two different profiles are established using random forest methodology, and predictive errors are identified. Mathematical equations lend support to the regression analysis. Segal (2004) explains that one of guiding forces in random forest regression is to increase variance by decreasing correlation.

To tune or not to tune the number of trees in random forest.
This article [@probst2018tune] intends to demonstrate whether to keep the number of trees in a random forest at the maximum feasible number, or to reduce the number of trees and the effects that might have.  Previous research has shown that past a point, increasing the number of trees yields a small gain in the area under the curve, but this research did not demonstrate whether there was a possibility of a smaller number of trees leading to a better result.  It was found that while there were specific situations and datasets where a lower tree count was beneficial, these were in the minority and selecting a larger number of trees is often beneficial with the possible exception of median squared/median absolute error rather than the more common mean squared/mean absolute error.

Improved random forest for classification.
The goal of [@paul2018improved] is to create a random forest model to improve feature selection as well as the optimal number of trees simultaneously.  This is done in iterations identifying important and unimportant features, then adds a set number of trees per pass until convergence.    The feature selection method appears to be similar to stepwise regression. Its not clear in the paper whether this method may have similar drawbacks to stepwise regression / feature selection.  The optimal number of trees to add is based on the probability of a good split for a given tree which is calculated using the strength and correlation of the forest.   The idea appears to be to limit the number of trees added to reduce the computational overhead, without a classification accuracy penalty. The results were a fast and accurate classifier with more automation for tuning than common random forest algorithms.

Random Forest Based Feature Induction
The goal of [@vens2011random] is to demonstrate a method to create induced features using a random forest, which has the benefit of handling sparse datasets and missing values by default. This can reduce dimensionality and assist with automated feature selection by reducing the dimensionality of the full dataset. When working with a very large number of features, some reduction is necessary to be able to conceptualize or easily work with the data. This method could also help with preparation for a method like PCA. A random forest is built similar to how one would be used for prediction, but instead of prediction the nodes of the trees are combined as a feature space. The resulting feature space used with a support vector machine was able to show high predictive performance, even when the original data set was not usable for SVM presumably due to missing values, On real datasets, this method often generated a better accuracy and in those cases where it did not, there was not a large penalty for using it with one exception.There may be limitations to use due to computational requirements, but there may also be datasets for which using this type of feature induction would decrease total computational requirements with some models.

Application of random forest algorithm on feature subset selection and classification and regression.
The goal of [@jaiswal2017application] is to use a random forest to select a subset of high value features to be analyzed, reducing dimensionality, computational overhead, reducing the probability of overfit, and possibly benefiting the modeler's ability to visualize and understand the dataset by removing unnecessary features.  The method used is to generate large, unpruned trees then use out of bag estimation and the importance of each variable is calculated based on the number of correct votes - the permuted out of bag variables. This can be an iterative process for large datasets.  Gini values are used to identify variable interactions across trees in the forest.  The results of this paper are not clear, but it does appear that there was a reduction in dimensionality using this method.   The limitations may be if there could be an engineered feature, these would be best created prior to using the random forest feature selection.

A guided random forest based feature selection approach for activity recognition.”
The purpose of [@uddin2015guided] is to use a random forest to during feature selection for a human activity recognition problem.This is performed by training a standard random forest then using the feature importance scores generated to select the most important features to use in a second random forest used for prediction.In general, feature selection of this type can improve accuracy and reduce computational complexity. The results were that the guided random forest has a comparable accuracy to the more common methods such as Relief-F and Lasso Logistic Regression, but has a lower computational complexity than Relief-F making it a possible choice for a solution requiring a tradeoff between complexity and accuracy.

A permutation importance-based feature selection method for short-term electricity load forecasting using random forest.
The goal of [@huang2016permutation] is to demonstrate a new feature selection method to be used with a random forest based on permutation importance values of a trained random forest to be used in selecting the best features to train a second random forest model to predict load forecasts for a power grid. This feature set is then used as a reduced feature set to train the short term load forecasting model, which improves the computation time necessary and may improve accuracy as well. This feature selection method provided worked best for a random forest model, but also provided better results for an artificial neural network and support vector regression indicating it may generalize well.

Overview of random forest methodology and practical guidance with emphasis on computational biology and bioinformatics.
[@Boulesteix2016random] Random forest (RF) methodology is found to be used to address two main classes of problems which is to construct a prediction rule in a supervised learning problem and to assess and rank variables with respect to their ability to predict the response. RF is a classification and regression method based on the aggregation of a large number of decision trees.  RF has become a popular analysis tool in many application fields including bioinformatics and will most probably remain relevant in the future due to its high flexibility. However, RF approaches still have to face a number of challenges. They can produce  unexpected results in some specific cases such as a bias depending on the distribution of the predictor. 

Comparison of Random Forest and SVM for Raw Data in Drug Discovery: Prediction of Radiation Protection and Toxicity Case Study.
[@Matsumoto2016comprison] Random forest and SVM was compared for the raw data in drug discovery. There were two types of problems based on the target protein. They predicted the radiation protection(cancer) function and toxicity for radioprotectors targeting p53 as a case study.Two experiments were performed for each compound.There were 84 total. First, experiments administering the compounds to normal cells were performed. These experiments were able to measure the toxicity.  Then, experiments administering the compounds to gamma
irradiated cells were performed. These experiments were able to measure the radiation-protection function. The experiment measured the cell death rate for each
concentration case. After using random forest and machine learning, SVM was found to be better than random forest as determined by the AUC score. In contrast, for predicting toxicity, random forest is better than SVM. 

Research on machine learning framework based on random forest algorithm.
[@Ren2017research] This article is about research on machine learning framework based on random forest algorithm.There is an introduction to the filtering method. Filtration method is through the statistical method which is to give the characteristics with a weight, to carry out feature ranking according to the characteristics of the weight.Then  some rules are applied to set a threshold and the feature whose weight is greater than the threshold value is retained or 
otherwise deleted. The steps are as follows for algorithm optimization which is to carry out feature selection and remove noise characteristics, carry out feature selection and delete redundant features, and voting strategies for optimizing the random forest algorithm.

Random Forest. [@Rigatti2017forest] Colon cancer data from the SEER database was  used to construct both a Cox model and a random forest model to determine how well the models perform on the same data.  The data set is sampled with bootstrap sampling. The article explains what random forest is and gives many examples.In the sample problem they evaluated , the Cox and random forest models performed similarly, so because the Cox model was easier to interpret , it made it the preferred method.It only had a a few predictors and no obvious interactions or nonlinear effects,  so the random forest model would not be the best option in this case.However, the both achieved well, because the  concordance error rate  of the models was approximately 18%.


# The chosen dataset
Data Source: **Mendeley Data**

[Species data used in Random Forest modelling to determine predictors of extinction](https://data.mendeley.com/datasets/tc6syk8vwf/1)

Published: 17 August 2023

Contributors:
Dewidine van der Colff, Sabrina Kumschick, Wendy Foden, Domitilla Raimondo, Christophe BOTELLA, Lize von Staden, John Wilson

**Description**

Data was extracted from the South African Red [@SANBI2021] List database to compile a profile for plant extinctions. South Africa offers a wide array of biodiversity and estimates over 22,000 plant taxa. The International Union for Conservation of Nature’s (IUCN) [@IUCN2023] Red List of Threatened Species provides a standardized method to document and assess extinctions (IUCN 2023). Species are classified into one of the following groups: Extinct (EX), Extinct in the wild (EW), Critically endangered possibly extinct (CR PE), Critically endangered (CR), Endangered (EN), Vulnerable (VU), Near threatened (NT), Conservation dependent (CD), Least concern (LC), and Data deficient (DD).

Plants are an essential component to an ecosystem’s functionality, so it is critical to evaluate drivers of extinction and determine methods of prevention. To examine potential indicators for extinctions, extinct, threatened, and non-threatened taxa are compared to identify and/or distinguish traits that may be associated with risk or vulnerability. The final dataset comprises 842 extant taxa, 33 Extinct taxa, and 69 Possibly Extinct (CR PE) taxa, to total 944 species. 

The table [@vanderColff2023] below organizes and summarizes the explanatory variables. 

| Class                | Predictor Variables       | Description                                                                                                                                                        |
|----------------------|---------------------------|--------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| **Human pressures**  | Habitat loss              | Conversion, fragmentation, and/or elimination of habitat. e.g., logging, wood harvesting, livestock farming, urban development.                                    |
|                      | Habitat degradation       | Alteration of natural habitats necessary for species survival resulting in reduced functionality e.g., fire suppression, droughts.                                  |
|                      | Invasive species          | Impacts of alien species on natives through different mechanisms e.g. alteration of soil chemistry, resource competition.                                          |
|                      | Pollution                 | Pollutants entering the natural environment e.g., air-borne pollutants, waste.                                                                                     |
|                      | Over-exploitation         | Excessive use of species causing decreases in viable populations e.g. overharvesting.                                                                              |
|                      | Other                     | Intrinsic factors, changes in native taxa dynamics, human disturbance, natural disasters.                                                                          |
|                      | Unknown                   | N/A                                                                                                                                                                |
| **Biological traits**| Life form (LF)            | Annual or perennial                                                                                                                                                |
|                      | Growth form (GF)          | One of 14 distinct forms: Parasitic plant, Tree, Shrub, Suffrutex, Herb, Lithophyte, Succulent, Graminoid, Geophyte, Climber, Carnivorous, Cyperoid, Creeper, Epiphyte. |
| **Distribution state**| Biomes                    | One of nine biomes present in South Africa: Fynbos, Grassland, Succulent Karoo, Albany Thicket, Savanna, Forest, Nama Karoo, Desert, Indian Ocean Coastal Belt. Note: if a taxon was found in multiple biomes, it was marked as generalist. |
|                      | Range size                | All species range sizes are based on the standard measure of Extent of Occurrence (EOO), a parameter defined as the shortest continuous imaginary boundary that can be drawn to encompass all the known, inferred, or projected sites of present occurrence of a taxon. |

# Analysis and Results
## Packages
The R packages utilized for running the statistical modeling for the Random Forest algorithm were **readr**, **plyr**, **ipred**, **caret**, **caTools**, **randomForest**, **ROSE**. 
- readr: Part of the tidyverse, readr is designed for reading rectangular data, particularly CSVs (comma-separated values) and other delimited types of text files. It's known for its speed and for providing more informative error messages compared to base R functions like read.csv. It also converts data into tibbles, which are a modern take on data frames.

- **plyr**: This package is used for splitting, applying, and combining data. plyr is known for its capability to handle different data types (arrays, lists, data frames, etc.) and apply functions to each element of the split data, then combine the results. Note that plyr is largely superseded by dplyr (also part of tidyverse), which is more efficient especially for large datasets.

- **ipred**: Standing for "Improved Predictors", ipred provides functions for predictive modeling. It includes methods for bagging (Bootstrap Aggregating), which helps improve the stability and accuracy of machine learning algorithms, particularly for decision trees.

- **caret**: The caret package (short for Classification And REgression Training) is a comprehensive framework for building machine learning models in R. It simplifies the process of model training, tuning, and predicting by providing a unified interface for various machine learning algorithms.

- **caTools**: This package contains several tools for handling data, including functions for reading/writing Binary Large Objects (BLOBs), moving window statistics, and splitting data into training/testing sets. It's often used for its simple and effective method for creating reproducible train/test splits.

- **randomForest**: As the name suggests, this package is used for implementing the Random Forest algorithm for classification and regression tasks. Random Forest is an ensemble learning method that operates by constructing a multitude of decision trees and outputting the mode of the classes (classification) or mean prediction (regression) of the individual trees.

- **ROSE**: Standing for Random OverSampling Examples, the ROSE package is used to deal with imbalanced datasets in binary classification problems. It generates synthetic samples in a two-class problem to balance the class distribution, using smoothed bootstrapping. This helps improve the performance of classification models on imbalanced datasets.

### Import packages
```{r}
library(readr)
library(plyr)
library(ipred)
library(caret)
library(caTools)
library(randomForest)
library(ROSE)
library(ggplot2)
library(knitr)
```
## Data and Visualization

The preview of the dataset

```{r, warning=FALSE, echo=T, message=FALSE}

data <- read_csv("All_threat_data.csv")

ggplot(data, aes(x = factor(Status), fill = factor(Status))) + 
  geom_bar(show.legend = FALSE) +
  scale_fill_brewer(palette = "Paired") +
  labs(title = "Barplot of Status", 
       x = "Status", 
       y = "Frequency") +
  theme_minimal() +
  theme(text = element_text(size = 12),
        plot.title = element_text(hjust = 0.5),
        axis.title = element_text(size = 14, face = "bold"),
        axis.text.x = element_text(angle = 45, hjust = 1),
        panel.grid.major = element_line(color = "grey80"),
        panel.grid.minor = element_blank())
```

## Data Preparation
Encode the 15 columns, with 'group' categorized into 1, 2, and 3. 'Yes' should be encoded as 1, 'No' as 0, and other categories should be numbered starting from 1. In the first column, replace the "-" with a ".".

### Original Dataset Preview

```{r}
original_data <- read_csv("All_threat_data.csv")
kable(head(original_data, 10))
```
### Encoding Dataset Prview

```{r, warning=FALSE, echo=TRUE}
encode_data <- read_csv("process.csv")
kable(head(encode_data, 10))
```
Split the dataset into a training set and a test set with a ratio of 7:3. Perform class imbalance handling on the training set using the ROSE library, aiming for an equal data quantity among classes, represented as 1:1:1.

Given a minority class sample point x, we find its k nearest neighbors. Then, we randomly select one of these neighbors, denoted as z, and construct a new data point y that lies on the line segment between x and z:

> y = x + λ * (z - x)

Here, λ is a random number between 0 and 1.

## Methods
The data needs to be normalized using 5 different methods (Min-Max Normalization, Z-Score Normalization, Max Absolute Scaling, L1 Norm Normalization, L2 Norm Normalization) due to significant differences in feature values initially.

> X_new = (X_old-min(X_old)) / (max(X_old)-min(X_old))
>
> X_new = (X_old - mean(X_old)) / sd(X_old)
>
> X_new = X_old / max(abs(X_old))
>
> X_new = X_old / sum(abs(X_old))
>
> X_new = X_old / sqrt(sum(X_old^2))

## Evaluation
Evaluate the test set and calculate four metrics (accuracy, recall, precision, F1).

- Accuracy = sum(actual labels == predicted labels) / total number of labels
- Recall = True Positives / (True Positives + False Negatives)
- Precision = True Positives / (True Positives + False Positives)
- F1 = 2 * (Precision * Recall) / (Precision + Recall)

## Final outcome:
The idea is to combine five models using a voting method and pick the category with the most votes as our final prediction. It's like a team effort in machine learning, known as the Hard Voting Classifier.
For a sample point x, five models make predictions respectively:

> y1 = Model1(x)
> 
> y2 = Model2(x)
> 
> y3 = Model3(x)
> 
> y4 = Model4(x)
> 
> y5 = Model5(x)

Place these five prediction results into a set `Y = {y1, y2, y3, y4, y5}`. The final prediction result `y_final` is the element that appears most frequently in the set `Y`, mathematically represented as `y_final = mode(Y)`.

## Statistical Modeling
### Data Processing
1. Process the data by setting the first 14 columns as [features] and the last column as the [label]
2. Split the dataset into training and testing sets
3. Combine the training datasets
4. Print the initial number of each category

```{r}
data <- read.csv("process.csv")

features <- data[, 1:14]
label <- data[, 15]

set.seed(42)

split <- sample.split(label, SplitRatio = 0.7)
features_train = features[split,]
features_test = features[!split,]
label_train = label[split]
label_test = label[!split]

data_train <- features_train
data_train$label <- label_train
class_counts <- table(data_train$label)

class_counts <- table(data_train$label)
print(paste("( Before )Data Category Counts: ", class_counts))
```
#### Handle class imbalance
1. Process classes A and B
2. Process classes A and C
3. Retain records in data_train_AB_resampled where the label is '2'
4. Retain records in data_train_AC_resampled where the label is '3'
5. Retain records in both data_train_AB_resampled and data_train_AC_resampled where the label is '1'
6. combine
7. Print the number of each category after class imbalance handling

```{r}
data_train_AB <- data_train
data_train_AB <- data_train_AB[data_train_AB$label != '3',]
data_train_AB_resampled <- ovun.sample(label ~ ., data = data_train_AB, method = "over", N = 980, seed = 1)$data

data_train_AC <- data_train
data_train_AC <- data_train_AC[data_train_AC$label != '2',]
data_train_AC_resampled <- ovun.sample(label ~ ., data = data_train_AC, method = "over", N = 980, seed = 1)$data

data_train_AB_2 <- data_train_AB_resampled[data_train_AB_resampled$label == '2',]
data_train_AC_3 <- data_train_AC_resampled[data_train_AC_resampled$label == '3',]

data_train_1 <- data_train_AB_resampled[data_train_AB_resampled$label == '1',]
data_train_combined <- rbind(data_train_1, data_train_AB_2, data_train_AC_3)

cat("( After )Data Category Counts:\n")
print(table(data_train_combined$label))
```
#### Normalization
1. Divide the features and label, and apply different normalization to the training and testing sets
2. Apply Min-Max normalization to features_train and features_test 
3. Apply Z-Score normalization to each column of features_train
4. features_test <- as.data.frame(mapply(function(x, y) {(x - mean(y))/sd(y)}, features_test, features_train, SIMPLIFY = FALSE))
5. Apply Max Absolute Value normalization to the training set
6. Apply L1 norm normalization to the training set
7. Apply L2 norm normalization to the training set

```{r}
features_train <- data_train_combined[, 1:14]
label <- data_train_combined[, 15]
```

```{r}
features_train <- as.data.frame(lapply(features_train, function(x) {(x-min(x))/(max(x)-min(x))}))
features_test <- as.data.frame(lapply(features_test, function(x) {(x-min(x))/(max(x)-min(x))}))

features_train_1 <- as.data.frame(lapply(features_train, function(x) {(x-min(x))/(max(x)-min(x))}))
features_test_1 <- as.data.frame(lapply(features_test, function(x) {(x-min(x))/(max(x)-min(x))}))

features_train_2 <- as.data.frame(lapply(features_train, function(x) {(x - mean(x))/sd(x)}))
features_test_2 <- as.data.frame(lapply(features_test, function(x) {(x - mean(x))/sd(x)}))

features_train_3 <- as.data.frame(lapply(features_train, function(x) {x / max(abs(x))}))
features_test_3 <- as.data.frame(lapply(features_test, function(x) {x / max(abs(x))}))

features_train_4 <- as.data.frame(lapply(features_train, function(x) {x / sum(abs(x))}))
features_test_4 <- as.data.frame(lapply(features_test, function(x) {x / sum(abs(x))}))

features_train_5 <- as.data.frame(lapply(features_train, function(x) {x / sqrt(sum(x^2))}))
features_test_5 <- as.data.frame(lapply(features_test, function(x) {x / sqrt(sum(x^2))}))
```
#### Train dataset
1. Training dataset after Min-Max normalization
2. Training dataset after Min-Max normalization
3. Training dataset after Z-Score normalization
4. Training dataset after Max Absolute Value normalization
5. Training dataset after L1 norm normalization
6. Training dataset after L2 norm normalization

```{r}
data_train <- features_train
data_train$label <- label
class_counts <- table(data_train$label)

data_train_1 <- features_train_1
data_train_1$label <- label
class_counts_1 <- table(data_train_1$label)

data_train_2 <- features_train_2
data_train_2$label <- label
class_counts_2 <- table(data_train_2$label)

data_train_3 <- features_train_3
data_train_3$label <- label
class_counts_3 <- table(data_train_3$label)

data_train_4 <- features_train_4
data_train_4$label <- label
class_counts_4 <- table(data_train_4$label)

data_train_5 <- features_train_5
data_train_5$label <- label
class_counts_5 <- table(data_train_5$label)
```
#### Model 1
1. Calculate and print the accuracy of the test set
2. Convert to a categorical variable
3. Obtain the confusion matrix
4. Calculate the average recall rate (Sensitivity)
5. Calculate the average precision rate (Positive Predictive Value)
6. print results

```{r}
model_1 <- randomForest(x = data_train_1[-ncol(data_train_combined)], y = as.factor(data_train_1$label), ntree = 2)
variable_importance_1 = importance(model_1)
pred_comb_1 <- predict(model_1, features_test_1)

accuracy_1 <- sum(label_test == pred_comb_1) / length(label_test)
print(paste('Accuracy of Min-Max Normalization for model1:', accuracy_1))

label_test_factor <- as.factor(label_test)
pred_comb_1_factor <- as.factor(pred_comb_1)

cm <- confusionMatrix(pred_comb_1_factor, label_test_factor)

recall <- mean(c(cm$byClass["Class: 1", "Sensitivity"], cm$byClass["Class: 2", "Sensitivity"], cm$byClass["Class: 3", "Sensitivity"]))

precision <- mean(c(cm$byClass["Class: 1", "Pos Pred Value"], cm$byClass["Class: 2", "Pos Pred Value"], cm$byClass["Class: 3", "Pos Pred Value"]))
F1 = 2 * recall * precision / ( recall + precision )

print(paste('Recall of model1 :', recall))
print(paste('Precision of model1 :', precision))
print(paste('F1 of model1 :', F1))
```

#### Model 2
1. Calculate and print the accuracy of the test set
2. Convert to a factor
3. Obtain the confusion matrix
4. Calculate the average recall rate (Sensitivity)
5. Calculate the average precision rate (Positive Predictive Value)
6. print results

```{r}
model_2 <- randomForest(x = data_train_2[-ncol(data_train_combined)], y = as.factor(data_train_2$label), ntree = 2) # nolint
variable_importance_2 = importance(model_2) # nolint
pred_comb_2 <- predict(model_2, features_test_2)

accuracy_2 <- sum(label_test == pred_comb_2) / length(label_test)
print(paste('Z-Score Normalization Accuracy for model2:', accuracy_2)) # nolint

label_test_factor <- as.factor(label_test)
pred_comb_1_factor <- as.factor(pred_comb_2)

cm <- confusionMatrix(pred_comb_1_factor, label_test_factor)

recall <- mean(c(cm$byClass["Class: 1", "Sensitivity"],  # nolint
                              cm$byClass["Class: 2", "Sensitivity"],  # nolint
                              cm$byClass["Class: 3", "Sensitivity"]))

precision <- mean(c(cm$byClass["Class: 1", "Pos Pred Value"], # nolint 
                            cm$byClass["Class: 2", "Pos Pred Value"], # nolint 
                            cm$byClass["Class: 3", "Pos Pred Value"]))
F1 = 2 * recall * precision / ( recall + precision ) # nolint

print(paste('Recall of model2 :', recall))  # nolint
print(paste('Precision of model2 :', precision))  # nolint
print(paste('F1 of model2 :', F1))  # nolint
```
#### Model 3
1. Calculate and print the accuracy of the test set
2. Convert to a factor
3. Obtain the confusion matrix
4. Calculate the average recall rate (Sensitivity)
5. Calculate the average precision rate (Positive Predictive Value)
6. print results

```{r}
model_3 <- randomForest(x = data_train_3[-ncol(data_train_combined)], y = as.factor(data_train_3$label), ntree = 2)
variable_importance_3 = importance(model_3)
pred_comb_3 <- predict(model_3, features_test_3)

accuracy_3 <- sum(label_test == pred_comb_3) / length(label_test)
print(paste('Accuracy of Max absolute value normalization for model3:', accuracy_3))

label_test_factor <- as.factor(label_test)
pred_comb_1_factor <- as.factor(pred_comb_3)

cm <- confusionMatrix(pred_comb_1_factor, label_test_factor)

recall <- mean(c(cm$byClass["Class: 1", "Sensitivity"],
                              cm$byClass["Class: 2", "Sensitivity"],
                              cm$byClass["Class: 3", "Sensitivity"]))

precision <- mean(c(cm$byClass["Class: 1", "Pos Pred Value"],
                            cm$byClass["Class: 2", "Pos Pred Value"],
                            cm$byClass["Class: 3", "Pos Pred Value"]))
F1 = 2 * recall * precision / ( recall + precision )

print(paste('Recall of model3 :', recall))
print(paste('Precision of model3 :', precision))
print(paste('F1 of mode3 :', F1))
```

#### Model 4
1. Calculate and print the accuracy of the test set
2. Convert to a factor
3. Obtain the confusion matrix
4. Calculate the average recall rate (Sensitivity)
5. Calculate the average precision rate (Positive Predictive Value)
6. print results

```{r}
model_4 <- randomForest(x = data_train_4[-ncol(data_train_combined)], y = as.factor(data_train_4$label), ntree = 2)
variable_importance_4 = importance(model_4)
pred_comb_4 <- predict(model_4, features_test_4)

accuracy_4 <- sum(label_test == pred_comb_4) / length(label_test)
print(paste('Accuracy of L1 norm normalization for model4:', accuracy_4))

label_test_factor <- as.factor(label_test)
pred_comb_1_factor <- as.factor(pred_comb_4)

cm <- confusionMatrix(pred_comb_1_factor, label_test_factor)

recall <- mean(c(cm$byClass["Class: 1", "Sensitivity"],
                              cm$byClass["Class: 2", "Sensitivity"],
                              cm$byClass["Class: 3", "Sensitivity"]))

precision <- mean(c(cm$byClass["Class: 1", "Pos Pred Value"],
                            cm$byClass["Class: 2", "Pos Pred Value"],
                            cm$byClass["Class: 3", "Pos Pred Value"]))
F1 = 2 * recall * precision / ( recall + precision )

print(paste('Recall of model4 :', recall))
print(paste('Precision of model4 :', precision))
print(paste('F1 of model4 :', F1))
```
##### Model 5
1. Calculate and print the accuracy of the test set
2. Convert to a factor
3. Obtain the confusion matrix
4. Calculate the average recall rate (Sensitivity)
5. Calculate the average precision rate (Positive Predictive Value)
6. print results

```{r}
model_5 <- randomForest(x = data_train_5[-ncol(data_train_combined)], y = as.factor(data_train_5$label), ntree = 2)
variable_importance_5 = importance(model_5)
pred_comb_5 <- predict(model_5, features_test_5)

accuracy_5 <- sum(label_test == pred_comb_5) / length(label_test)
print(paste('Accuracy of L2 norm normalization for model5:', accuracy_5))

label_test_factor <- as.factor(label_test)
pred_comb_1_factor <- as.factor(pred_comb_5)

cm <- confusionMatrix(pred_comb_1_factor, label_test_factor)

recall <- mean(c(cm$byClass["Class: 1", "Sensitivity"],
                              cm$byClass["Class: 2", "Sensitivity"],
                              cm$byClass["Class: 3", "Sensitivity"]))

precision <- mean(c(cm$byClass["Class: 1", "Pos Pred Value"],
                            cm$byClass["Class: 2", "Pos Pred Value"],
                            cm$byClass["Class: 3", "Pos Pred Value"]))
F1 = 2 * recall * precision / ( recall + precision )

print(paste('Recall of model5 :', recall))
print(paste('Precision of model5 :', precision))
print(paste('F1 of model5 :', F1))
```
#### Prediction
1. Obtain the number of predicted results
2. Initialize an empty vector to store the final prediction results
3. Iterate over each test sample
4. Get the prediction results of the five models for the i-th sample 
5. Select the most frequently predicted class as the final prediction result for the i-th sample
6. Now final_pred contains the prediction results after voting
7. Calculate and print the accuracy
8. Convert to a factor type
9. Obtain the confusion matrix
10. Calculate the recall (Sensitivity) for each category
11. Calculate the precision for each category

```{r}

n <- length(pred_comb_1)

final_pred <- rep(NA, n)

for(i in 1:n) {
   preds <- c(pred_comb_1[i], pred_comb_2[i], pred_comb_3[i], pred_comb_4[i], pred_comb_5[i])

   final_pred[i] <- as.numeric(names(which.max(table(preds))))
}

importances_list <- list(variable_importance_1, variable_importance_2, variable_importance_3, variable_importance_4, variable_importance_5)
average_importance <- Reduce("+", importances_list) / length(importances_list)
print(average_importance)

accuracy <- sum(label_test == final_pred) / length(label_test)
print(paste('Accuracy of Voting method:', accuracy))

final_pred_factor <- as.factor(final_pred)
label_test_factor <- as.factor(label_test)

cm_vote <- confusionMatrix(final_pred_factor, label_test_factor)

sensitivity_class1 <- cm_vote$byClass["Class: 1", "Sensitivity"]
sensitivity_class2 <- cm_vote$byClass["Class: 2", "Sensitivity"]
sensitivity_class3 <- cm_vote$byClass["Class: 3", "Sensitivity"]
recall = (sensitivity_class1 + sensitivity_class2 + sensitivity_class3) / 3
print(paste('Recall :', recall))

precision_class1 <- cm_vote$byClass["Class: 1", "Pos Pred Value"]
precision_class2 <- cm_vote$byClass["Class: 2", "Pos Pred Value"]
precision_class3 <- cm_vote$byClass["Class: 3", "Pos Pred Value"]
precision = (precision_class1 + precision_class2 + precision_class3) / 3
print(paste('Precision :', precision))

F1 = 2 * recall * precision / ( recall + precision )
print(paste('F1 :', F1))
```
## Conclusion
# TODO
# TODO: Add references of Hard Voting Classifier
## References